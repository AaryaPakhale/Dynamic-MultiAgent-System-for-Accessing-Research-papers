{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/8b/c8/990e22a465e4771338da434d799578865d6d7ef1fdb50bd844b7ecdcfa19/sentence_transformers-3.3.1-py3-none-any.whl.metadata\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.41.0 from https://files.pythonhosted.org/packages/45/d6/a69764e89fc5c2c957aa473881527c8c35521108d553df703e9ba703daeb/transformers-4.48.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers) (2.5.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/25/92/ee1d7a00bb6b8c55755d4984fd82608603a3cc59959245068ce32e7fb808/scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/81/8c/ab85f1aa1cc200c796532a385b6ebf6a81089747adc1da7482a062acc46c/scipy-1.15.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scipy-1.15.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m698.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.20.0 from https://files.pythonhosted.org/packages/6c/3f/50f6b25fafdcfb1c089187a328c95081abf882309afd86f4053951507cd1/huggingface_hub-0.27.1-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Obtaining dependency information for Pillow from https://files.pythonhosted.org/packages/d9/45/3fe487010dd9ce0a06adf9b8ff4f273cc0a44536e234b0fad3532a42c15b/pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Collecting numpy>=1.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for numpy>=1.17 from https://files.pythonhosted.org/packages/9f/fd/2279000cf29f58ccfd3778cbf4670dfe3f7ce772df5e198c5abe9e88b7d7/numpy-2.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-2.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (116 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/c5/1b/f0e4d13e6adf866ce9b069e191f303a30ab1277e037037a365c3aad5cc9c/regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/22/7a/88e58bb297c22633ed1c9d16029316e5b5ac5ee44012164c2edede599a5e/tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/24/84/e9d3ff57ae50dd0028f301c9ee064e5087fe8b00e55696677a0413c377a7/safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Downloading scipy-1.15.1-cp311-cp311-macosx_12_0_arm64.whl (32.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.5/32.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached numpy-2.2.1-cp311-cp311-macosx_11_0_arm64.whl (14.4 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Installing collected packages: tqdm, threadpoolctl, safetensors, regex, Pillow, numpy, joblib, scipy, huggingface-hub, tokenizers, scikit-learn, transformers, sentence-transformers\n",
      "Successfully installed Pillow-11.1.0 huggingface-hub-0.27.1 joblib-1.4.2 numpy-2.2.1 regex-2024.11.6 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.3.1 threadpoolctl-3.5.0 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/d0/a8/0a8f868615b7a30636b1d15b718e3ea9875bf0dccced03583477c2372495/langchain-0.3.14-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/4c/f5/8cce9196434014a24cc65f6c68faa9a887080932361ee285986c0a35892d/SQLAlchemy-2.0.37-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached SQLAlchemy-2.0.37-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Obtaining dependency information for aiohttp<4.0.0,>=3.8.3 from https://files.pythonhosted.org/packages/a9/60/03455476bf1f467e5b4a32a465c450548b2ce724eec39d69f737191f936a/aiohttp-3.11.11-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached aiohttp-3.11.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in ./.venv/lib/python3.11/site-packages (from langchain) (0.3.29)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Obtaining dependency information for langchain-text-splitters<0.4.0,>=0.3.3 from https://files.pythonhosted.org/packages/4b/83/f8081c3bea416bd9d9f0c26af795c74f42c24f9ad3c4fbf361b7d69de134/langchain_text_splitters-0.3.5-py3-none-any.whl.metadata\n",
      "  Using cached langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in ./.venv/lib/python3.11/site-packages (from langchain) (0.2.10)\n",
      "Collecting numpy<2,>=1.22.4 (from langchain)\n",
      "  Obtaining dependency information for numpy<2,>=1.22.4 from https://files.pythonhosted.org/packages/1a/2e/151484f49fd03944c4a3ad9c418ed193cfd02724e138ac8a9505d056c582/numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.venv/lib/python3.11/site-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain) (9.0.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for aiohappyeyeballs>=2.3.0 from https://files.pythonhosted.org/packages/b9/74/fbb6559de3607b3300b9be3cc64e97548d55678e44623db17820dbd20002/aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for attrs>=17.3.0 from https://files.pythonhosted.org/packages/89/aa/ab0f7891a01eeb2d2e338ae8fecbe57fcebea1a24dbb64d45801bfab481d/attrs-24.3.0-py3-none-any.whl.metadata\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/2c/31/ab01375682f14f7613a1ade30149f684c84f9b8823a4391ed950c8285656/frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/70/0f/6dc70ddf5d442702ed74f298d69977f904960b82368532c88e854b79f72b/multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for propcache>=0.2.0 from https://files.pythonhosted.org/packages/3c/09/8386115ba7775ea3b9537730e8cf718d83bbf95bffe30757ccf37ec4e5da/propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.17.0 from https://files.pythonhosted.org/packages/5a/a1/205ab51e148fdcedad189ca8dd587794c6f119882437d04c33c01a75dece/yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
      "Using cached aiohttp-3.11.11-cp311-cp311-macosx_11_0_arm64.whl (455 kB)\n",
      "Using cached langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached SQLAlchemy-2.0.37-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl (52 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl (92 kB)\n",
      "Installing collected packages: SQLAlchemy, propcache, numpy, multidict, frozenlist, attrs, aiohappyeyeballs, yarl, aiosignal, aiohttp, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "Successfully installed SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 attrs-24.3.0 frozenlist-1.5.0 langchain-0.3.14 langchain-text-splitters-0.3.5 multidict-6.1.0 numpy-1.26.4 propcache-0.2.1 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting httpx==0.23.0\n",
      "  Obtaining dependency information for httpx==0.23.0 from https://files.pythonhosted.org/packages/e9/fd/d8ff4bbf7ade1c9d60b53bf8234b44dcb2a9fcc7ae6933ae80ba38582f3e/httpx-0.23.0-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.23.0-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx==0.23.0) (2024.12.14)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from httpx==0.23.0) (1.3.1)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3 (from httpx==0.23.0)\n",
      "  Obtaining dependency information for rfc3986[idna2008]<2,>=1.3 from https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore<0.16.0,>=0.15.0 (from httpx==0.23.0)\n",
      "  Obtaining dependency information for httpcore<0.16.0,>=0.15.0 from https://files.pythonhosted.org/packages/ad/b9/260603ca0913072a10a4367c2dca9998706812a8c1f4558eca510f85ae16/httpcore-0.15.0-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-0.15.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting h11<0.13,>=0.11 (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0)\n",
      "  Obtaining dependency information for h11<0.13,>=0.11 from https://files.pythonhosted.org/packages/60/0f/7a0eeea938eaf61074f29fed9717f2010e8d0e0905d36b38d3275a1e4622/h11-0.12.0-py3-none-any.whl.metadata\n",
      "  Using cached h11-0.12.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting anyio==3.* (from httpcore<0.16.0,>=0.15.0->httpx==0.23.0)\n",
      "  Obtaining dependency information for anyio==3.* from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx==0.23.0) (3.10)\n",
      "Using cached httpx-0.23.0-py3-none-any.whl (84 kB)\n",
      "Using cached httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "Using cached rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Installing collected packages: rfc3986, h11, anyio, httpcore, httpx\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.8.0\n",
      "    Uninstalling anyio-4.8.0:\n",
      "      Successfully uninstalled anyio-4.8.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "Successfully installed anyio-3.7.1 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 rfc3986-1.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain-ollama\n",
      "  Obtaining dependency information for langchain-ollama from https://files.pythonhosted.org/packages/7f/77/219fb2290c832e33af2731246ea3328bade50756288c1e97ae73c4ccc197/langchain_ollama-0.2.2-py3-none-any.whl.metadata\n",
      "  Using cached langchain_ollama-0.2.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in ./.venv/lib/python3.11/site-packages (from langchain-ollama) (0.3.29)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
      "  Obtaining dependency information for ollama<1,>=0.4.4 from https://files.pythonhosted.org/packages/93/71/44e508b6be7cc12efc498217bf74f443dbc1a31b145c87421d20fe61b70b/ollama-0.4.5-py3-none-any.whl.metadata\n",
      "  Using cached ollama-0.4.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (4.12.2)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama<1,>=0.4.4->langchain-ollama)\n",
      "  Obtaining dependency information for httpx<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (3.7.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain-ollama)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.3.0)\n",
      "Using cached langchain_ollama-0.2.2-py3-none-any.whl (18 kB)\n",
      "Using cached ollama-0.4.5-py3-none-any.whl (13 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx, ollama, langchain-ollama\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.12.0\n",
      "    Uninstalling h11-0.12.0:\n",
      "      Successfully uninstalled h11-0.12.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.15.0\n",
      "    Uninstalling httpcore-0.15.0:\n",
      "      Successfully uninstalled httpcore-0.15.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.23.0\n",
      "    Uninstalling httpx-0.23.0:\n",
      "      Successfully uninstalled httpx-0.23.0\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 langchain-ollama-0.2.2 ollama-0.4.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -qqq install langchain-groq --progress-bar off\n",
    "!pip -qqq install torch --progress-bar off\n",
    "!pip -qqq install markdown --progress-bar off\n",
    "!pip -qqq install beautifulsoup4 --progress-bar off\n",
    "!pip install sentence-transformers\n",
    "!pip install langchain\n",
    "!pip install httpx==0.23.0\n",
    "!pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-together\n",
      "  Obtaining dependency information for langchain-together from https://files.pythonhosted.org/packages/a8/69/b3cbcf5b43acbc098c012ef75035fb0dc1e0f227f5161329ef9884a25ba4/langchain_together-0.3.0-py3-none-any.whl.metadata\n",
      "  Using cached langchain_together-0.3.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in ./.venv/lib/python3.11/site-packages (from langchain-together) (3.11.11)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in ./.venv/lib/python3.11/site-packages (from langchain-together) (0.3.29)\n",
      "Collecting langchain-openai<0.4,>=0.3 (from langchain-together)\n",
      "  Obtaining dependency information for langchain-openai<0.4,>=0.3 from https://files.pythonhosted.org/packages/4a/9c/b38e308ac668f6db067b424a2a78e5b865753c144a119456f008a09230db/langchain_openai-0.3.0-py3-none-any.whl.metadata\n",
      "  Using cached langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain-together) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-together) (1.18.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-together) (4.12.2)\n",
      "Collecting openai<2.0.0,>=1.58.1 (from langchain-openai<0.4,>=0.3->langchain-together)\n",
      "  Obtaining dependency information for openai<2.0.0,>=1.58.1 from https://files.pythonhosted.org/packages/70/45/6de8e5fd670c804b29c777e4716f1916741c71604d5c7d952eee8432f7d3/openai-1.59.6-py3-none-any.whl.metadata\n",
      "  Using cached openai-1.59.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai<0.4,>=0.3->langchain-together)\n",
      "  Obtaining dependency information for tiktoken<1,>=0.7 from https://files.pythonhosted.org/packages/8c/f8/f0101d98d661b34534769c3818f5af631e59c36ac6d07268fbfc89e539ce/tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-together) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-together) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-together) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain-together) (2024.12.14)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-together) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain-together) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain-together) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain-together)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/f5/97/0468bc9eeae43079aaa5feb9267964e496bf13133d469cfdc135498f8dd0/jiter-0.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached jiter-0.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain-together) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.4,>=0.3->langchain-together) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain-together) (2.27.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai<0.4,>=0.3->langchain-together) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain-together) (0.14.0)\n",
      "Using cached langchain_together-0.3.0-py3-none-any.whl (12 kB)\n",
      "Using cached langchain_openai-0.3.0-py3-none-any.whl (54 kB)\n",
      "Using cached openai-1.59.6-py3-none-any.whl (454 kB)\n",
      "Using cached tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl (982 kB)\n",
      "Using cached jiter-0.8.2-cp311-cp311-macosx_11_0_arm64.whl (311 kB)\n",
      "Installing collected packages: jiter, tiktoken, openai, langchain-openai, langchain-together\n",
      "Successfully installed jiter-0.8.2 langchain-openai-0.3.0 langchain-together-0.3.0 openai-1.59.6 tiktoken-0.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_groq import ChatGroq\n",
    "import json\n",
    "import markdown\n",
    "import bs4\n",
    "from typing import Dict, List, Any\n",
    "import yaml\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import lru_cache\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import asyncio\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_together import ChatTogether\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, tokens_per_minute: int = 5000):\n",
    "        self.capacity = tokens_per_minute\n",
    "        self.tokens = tokens_per_minute\n",
    "        self.last_updated = datetime.now()\n",
    "        self.tokens_per_minute = tokens_per_minute\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def get_tokens(self, requested_tokens: int) -> bool:\n",
    "        async with self.lock:\n",
    "            now = datetime.now()\n",
    "            time_passed = (now - self.last_updated).total_seconds() / 60.0\n",
    "            \n",
    "            # Refill tokens based on time passed\n",
    "            self.tokens = min(\n",
    "                self.capacity,\n",
    "                self.tokens + (self.tokens_per_minute * time_passed)\n",
    "            )\n",
    "            self.last_updated = now\n",
    "\n",
    "            if self.tokens >= requested_tokens:\n",
    "                self.tokens -= requested_tokens\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    async def wait_for_tokens(self, requested_tokens: int):\n",
    "        while not await self.get_tokens(requested_tokens):\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "class PaperContentExtractor:\n",
    "    def __init__(self, groq_api_key: str, max_workers: int = 3, cache_dir: str = None):\n",
    "        self.llm = ChatGroq(\n",
    "            temperature=0,\n",
    "            api_key=\"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\",\n",
    "            model_name=\"llama-3.1-70b-versatile\"\n",
    "        )\n",
    "        self.max_workers = max_workers\n",
    "        self.token_bucket = TokenBucket(tokens_per_minute=5000)\n",
    "        self.cache_dir = cache_dir or os.path.join(os.getcwd(), 'feature_cache')\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        self.section_keywords = {\n",
    "            'introduction': ['introduction', 'background', 'overview', 'abstract'],\n",
    "            'methodology': ['method', 'approach', 'implementation', 'proposed', 'architecture', 'system', 'model'],\n",
    "            'results': ['result', 'evaluation', 'experiment', 'performance', 'analysis', 'finding'],\n",
    "            'conclusion': ['conclusion', 'discussion', 'future', 'summary']\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_section_hash(content: str) -> str:\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def _split_by_sections(self, content: str) -> List[tuple]:\n",
    "        sections = []\n",
    "        lines = content.split('\\n')\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            \n",
    "            # Check for markdown headers (# style)\n",
    "            header_match = re.match(r'^#+\\s+(.+)$', line)\n",
    "            if header_match:\n",
    "                if current_section and current_content:\n",
    "                    sections.append((current_section, '\\n'.join(current_content)))\n",
    "                current_section = header_match.group(1).strip()\n",
    "                current_content = []\n",
    "            elif line and current_section is not None:\n",
    "                current_content.append(line)\n",
    "            elif line and not sections:  # Content before first heading\n",
    "                current_section = \"Introduction\"\n",
    "                current_content.append(line)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # Add the last section\n",
    "        if current_section and current_content:\n",
    "            sections.append((current_section, '\\n'.join(current_content)))\n",
    "        \n",
    "        print(f\"Found sections: {[section[0] for section in sections]}\")\n",
    "        return sections\n",
    "\n",
    "    def _get_cache_path(self, section_hash: str) -> str:\n",
    "        return os.path.join(self.cache_dir, f\"{section_hash}.json\")\n",
    "\n",
    "    def _load_from_cache(self, section_hash: str) -> Dict:\n",
    "        cache_path = self._get_cache_path(section_hash)\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                with open(cache_path, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def _save_to_cache(self, section_hash: str, features: Dict):\n",
    "        cache_path = self._get_cache_path(section_hash)\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(features, f)\n",
    "\n",
    "    def _get_empty_features(self) -> Dict:\n",
    "        return {\n",
    "            \"Topic of Research\": \"\",\n",
    "            \"Research Objective\": \"\",\n",
    "            \"Methodology\": \"\",\n",
    "            \"Results\": \"\",\n",
    "            \"Novelty Claims\": \"\",\n",
    "            \"Evaluation Metrics\": \"\",\n",
    "            \"Category of Research\": \"\"\n",
    "        }\n",
    "\n",
    "    async def _extract_section_features(self, section: str) -> Dict:\n",
    "        # Estimate tokens (rough approximation)\n",
    "        estimated_tokens = len(section.split()) * 1.5\n",
    "        \n",
    "        await self.token_bucket.wait_for_tokens(estimated_tokens)\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this research paper section and extract key information.\n",
    "        Return a JSON object with these keys (use empty string if information is not found):\n",
    "        {{\n",
    "            \"Topic of Research\": \"The main research topic or focus area\",\n",
    "            \"Research Objective\": \"The specific goals or objectives\",\n",
    "            \"Methodology\": \"Methods, approaches, or techniques used\",\n",
    "            \"Results\": \"Key findings or outcomes\",\n",
    "            \"Novelty Claims\": \"Claims about new contributions or innovations\",\n",
    "            \"Evaluation Metrics\": \"Metrics used to evaluate results\",\n",
    "            \"Category of Research\": \"Type of research (e.g., empirical, theoretical, applied)\"\n",
    "        }}\n",
    "        STRICTLY return in JSON format only. No other text\n",
    "        Section content:\n",
    "        {section}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            print(response)\n",
    "            try:\n",
    "                return json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to parse JSON response: {response.content[:200]}...\")\n",
    "                return self._get_empty_features()\n",
    "        except Exception as e:\n",
    "            if \"rate_limit_exceeded\" in str(e):\n",
    "                print(\"Rate limit exceeded, waiting before retry...\")\n",
    "                await asyncio.sleep(2)\n",
    "                return await self._extract_section_features(section)\n",
    "            print(f\"Error in LLM call: {str(e)}\")\n",
    "            return self._get_empty_features()\n",
    "\n",
    "    async def _process_section(self, section_tuple: tuple) -> Dict:\n",
    "        header, content = section_tuple\n",
    "        if len(content.strip()) < 50:\n",
    "            return self._get_empty_features()\n",
    "        \n",
    "        section_hash = self._get_section_hash(content)\n",
    "        \n",
    "        # Try to load from cache first\n",
    "        cached_features = self._load_from_cache(section_hash)\n",
    "        if cached_features:\n",
    "            return cached_features\n",
    "        \n",
    "        features = await self._extract_section_features(content)\n",
    "        self._save_to_cache(section_hash, features)\n",
    "        \n",
    "        print(f\"Processed section: {header[:50]}...\")\n",
    "        return features\n",
    "\n",
    "    def _merge_features(self, features_list: List[Dict]) -> Dict:\n",
    "        if not features_list:\n",
    "            return self._get_empty_features()\n",
    "            \n",
    "        merged = self._get_empty_features()\n",
    "        \n",
    "        for key in merged:\n",
    "            values = [f[key] for f in features_list if f[key] and f[key].strip()]\n",
    "            if values:\n",
    "                merged[key] = max(values, key=len)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    async def extract_paper_features(self, file_path: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                content = f.read()\n",
    "        \n",
    "        sections = self._split_by_sections(content)\n",
    "        print(f\"Found {len(sections)} sections in {file_path}\")\n",
    "        \n",
    "        features_list = []\n",
    "        tasks = [self._process_section(section) for section in sections]\n",
    "        features_list = await asyncio.gather(*tasks)\n",
    "        \n",
    "        merged_features = self._merge_features([f for f in features_list if any(f.values())])\n",
    "        return merged_features\n",
    "\n",
    "async def process_papers(papers_dir: str, output_dir: str, groq_api_key: str = None, max_workers: int = 3):\n",
    "    if not groq_api_key:\n",
    "        raise ValueError(\"Please provide GROQ_API_KEY\")\n",
    "            \n",
    "    extractor = PaperContentExtractor(groq_api_key, max_workers)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    columns = [\n",
    "        \"Paper Code\", \"Topic of Research\", \"Research Objective\", \"Methodology\",\n",
    "        \"Results\", \"Novelty Claims\", \"Evaluation Metrics\",\n",
    "        \"Category of Research\"\n",
    "    ]\n",
    "    \n",
    "    paper_data = []\n",
    "    filenames = []\n",
    "    md_files = [f for f in os.listdir(papers_dir) if f.endswith('.md')]\n",
    "    \n",
    "    if not md_files:\n",
    "        print(f\"No markdown files found in {papers_dir}\")\n",
    "        return pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for filename in tqdm(md_files, desc=\"Processing papers\"):\n",
    "        paper_path = os.path.join(papers_dir, filename)\n",
    "        output_path = os.path.join(output_dir, f\"{filename[:-3]}_features.yaml\")\n",
    "        \n",
    "        try:\n",
    "            features = await extractor.extract_paper_features(paper_path)\n",
    "            paper_data.append(features)\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                yaml.dump(features, f)\n",
    "                \n",
    "            print(f\"Successfully processed {filename}\")\n",
    "            filenames.append(filename)\n",
    "            df_paper = pd.DataFrame(paper_data, columns=columns)\n",
    "            df_paper['filename'] = filenames\n",
    "            df_paper.to_csv('paper_analysis_results.csv', index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    return df_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 7/7 [00:00<00:00, 106.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sections: ['Abstract', '1 Introduction', '2 Related Work', '3 Methodology', '1. Introduction', '4. Experiments', '4.1 Comparing Different Basis Expansions', '4. Key Takeaways', '4.2 The Necessity of Ensembles of Dynamic Ensembles', 'Key Takeaway', '4.3 E-DOEBE Outperforms Other Methods', 'Key Takeaway', '5 Conclusion', '6 Tables']\n",
      "Found 14 sections in /Users/avinandan/Desktop/KDSH Task 1/P009_parsed.md\n",
      "Successfully processed P009_parsed.md\n",
      "Found sections: ['Abstract', '1 Introduction', '2 Related Work', 'Theoretical Models of Entropy', 'Methodology', 'Research Findings on Entropy and Complex Systems', 'Research Insights on Entropy and Its Applications', 'Experiments', 'Research Journey', '5 Results', 'Table 3: Entropy levels in various systems', 'Conclusion']\n",
      "Found 12 sections in /Users/avinandan/Desktop/KDSH Task 1/P119_parsed.md\n",
      "Successfully processed P119_parsed.md\n",
      "Found sections: ['Abstract', '1 Introduction', '2 Related Work', '3 Baseline Parsing Pipeline', '3.1 Segmentation and Feature Extraction', '3.2 Label Likelihood Estimation', '3.3 Smoothing and Inference', 'Improving Superpixel Label Costs', 'Fusing Classifiers', '4.2 Normalized Weight Learning', '5 Scene-Level Global Context', '5.1 Context-Aware Global Label Costs', '5.2 Inference with Label Costs', '6 Experiments', '6.1 Results', '6.2 Running Time', '6.3 Discussion', '7 Conclusion']\n",
      "Found 18 sections in /Users/avinandan/Desktop/KDSH Task 1/P025_parsed.md\n",
      "Successfully processed P025_parsed.md\n",
      "Found sections: ['Abstract', '1 Introduction', 'A novel approach employing graph neural networks for the identification and explication of hate speech directed at Islam (XG-HSI)', '2 Background', '3.1 Notations', '3.2 Data Pre-Processing', '3.3 Graph Encoder', '3.4 Loss Function', '3.5 Graph Explanation', '4.1 Experimental Setup', 'Implementation Details', '4.2 Experimental Results', '5 Graph Explanation Case Study', '6 Discussion', '7 Conclusion', 'Limitations', 'Ethical Implications', 'Societal Implications', 'Potential Risks', 'Acknowledgements']\n",
      "Found 20 sections in /Users/avinandan/Desktop/KDSH Task 1/P031_parsed.md\n",
      "Successfully processed P031_parsed.md\n",
      "Found sections: ['Abstract', '1 Introduction', '2.1.1 Unsupervised Objectives', '2.1.2 Conditional and Attribute-variant Objectives', '2.2 Neural Architectures', '2.3 Evaluation of Disentanglement', '2.4.1 Controlled Capacity Increase', '2.4.2 Reconstruction Weight Scheduler', '2.4.3 Dynamic Learning Rate Scheduling', '2.4.4 Logging and Visualization', '3 Experiments and Results', 'Table 1: Results of the best configurations of β-TCVAE on DCI, FactorVAE, SAP, MIG, and IRS metrics.', '4 Conclusion', 'Appendix A. Latent Factor Traversal']\n",
      "Found 14 sections in /Users/avinandan/Desktop/KDSH Task 1/P082_parsed.md\n",
      "Successfully processed P082_parsed.md\n",
      "Found sections: ['Abstract', '1 Introduction', '2 Methodology', '2.1 Environment Modeling', '2.2 Masked Environment Model', '2.3 Counterfactual Future Advantage', '2.4 Summary of MBCAL', '3.1 Datasets', '3.2 Experimental Settings', '3.2.1 Evaluation Settings', '3.3 Methods for Comparison', '3.4.1 Results of Batch-RL Evaluation', 'Table 1: Average reward per session of different algorithms and datasets in Batch-RL evaluation.', 'Table 2: Properties of Datasets and Simulators.', '3.4.2 Results of Growing Batch-RL Evaluation', '3.4.3 Analysis of the variance', 'Table 3: The mean square error (MSE) loss of different algorithms in different environments.', '4 Conclusion']\n",
      "Found 18 sections in /Users/avinandan/Desktop/KDSH Task 1/P010_parsed.md\n",
      "Successfully processed P010_parsed.md\n",
      "Found sections: ['Abstract', '1 Introduction', 'Notations', 'Transductive Node Classification', 'Graph Neural Networks', 'LaF is Admissible, but Not Explored Well', '4 LaF Strengthens the Expressive Power of GNNs', 'Theorem 4.1.', 'Proof.', 'Proposition 4.2.', 'Proof.', '5 Training-free Graph Neural Networks', 'Definition 5.1 (Training-free Model).', 'Proposition 5.2.', 'Proof.', '6.1 Experimental Setup', '6.2 TFGNNs Outperform Existing GNNs in Training-free Setting', '6.3 Deep TFGNNs Perform Better in Training-free Setting', '6.4 TFGNNs Converge Fast', '6.5 TFGNNs are Robust to Feature Noise', '7.1 Labels as Features and Training-free GNNs', '7.2 Speeding up GNNs', '7.3 Expressive Power of GNNs', '8 Limitations', '9 Conclusion']\n",
      "Found 25 sections in /Users/avinandan/Desktop/KDSH Task 1/P004_parsed.md\n",
      "Successfully processed P004_parsed.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PAPERS_DIR = \"Parsed_Docs/Papers\"  # Directory containing markdown papers\n",
    "OUTPUT_DIR = \"target\"  # Directory for output features\n",
    "df = await process_papers(\n",
    "    PAPERS_DIR, \n",
    "    OUTPUT_DIR, \n",
    "    groq_api_key=\"<GROQ_API_KEY>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
